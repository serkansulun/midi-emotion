python train.py --debug --log_step 100 --n_layer 8 --max_step 300 --no_amp --attn_type causal-linear

8 layers

CLUSTER - RTX 6000
SRUN is the same with SBATCH
AMP
ms/batch  483 | ms/sample  121, max_cached: 7698, nvsmi: 8548
NO AMP
ms/batch  715 | ms/sample  179, max_cached: 6964, nvsmi: 7814

CLUSTER - RTX 2080TI
AMP
ms/batch  459 | ms/sample  115, max_cached: 7698, nvsmi: 8538
NO AMP
ms/batch  697 | ms/sample  174, max_cached: 6964, nvsmi: 7804

PASCAL
AMP
ms/batch 1136 | ms/sample  284, max_cached: 9334, nvsmi: 10301

NO AMP
FULL ATTENTION (default)
ms/batch 1115 | ms/sample  279, max_cached: 9498, nvsmi: 10465
CAUSAL-LINEAR ATTENTION ***
ms/batch  555 | ms/sample  139, max_cached: 3918, nvsmi: 4887 ***
REFORMER ATTENTION
ms/batch  908 | ms/sample  227, max_cached: 8016, nvsmi: 8985
LOCAL ATTENTION (256)
ms/batch  641 | ms/sample  160, max_cached: 5356, nvsmi: 6325

n_params for fast:    58059503
n_params for default: 58844399

Generation time: 4096 tokens. 252 seconds
0 arousal: 56 seconds